tokenizer:
  model_dir: output/v2/pretrain_protein/aa_tokenizer_bpe_pieces_3
  truncate: True
  max_length: 512
dataset:
  train:
    files:
      - data/proc/v8/pretrain_protein/aa_sequence_pieces_3.txt
    loader:
      batch_size: 64
      shuffle: True
      pin_memory: True
model:
  add_pooler: true
  attention_probs_dropout_prob: 0.1
  hidden_dropout_prob: 0.1
  hidden_size: 256
  initializer_range: 0.03
  intermediate_size: 1024
  num_attention_heads: 8
  num_hidden_layers: 6
  vocab_size: 9689
  pooler_type: cls_before_pooler  # "cls", "cls_before_pooler", "avg", "avg_top2", "avg_first_last"
  temperature: 0.05
trainer:
  num_epochs: 3
  optimizer:
    lr: 3.e-5
    weight_decay: 1.e-3
  scheduler:
    name: constant_with_warmup  # "constant" "linear" "cosine" "cosine_with_restarts" "polynomial" "constant" "constant_with_warmup" "inverse_sqrt"
    params:
      num_training_steps: 58500  # 187182//32+1=5850, *3~=17550
      num_warmup_steps: 1000
  print_per_steps: 1000
  eval_per_steps: 1000
  save_per_steps: 1000
  patience: 5
  max_keep: 2
  max_keep_best: 2
gpu: 0
pretrain_model_path: output/v2/pretrain_protein/aa_encoder_lyr3_lr0.0002_hdsz256_ir0.03/model_57000_6.2227.pt
model_dir: output/v2/pretrain_protein/aa_encoder_simcse
