tokenizer:
  model_dir: output/pretrain_drug/smiles_tokenizer_bpe
  truncate: True
  max_length: 256
mlm:
  mask_rate: 0.15
  mask_token_rate: 0.8
  random_token_rate: 0.1
dataset:
  train:
    files:
      - data/proc/pretrain_drug/smiles_train.txt
    loader:
      batch_size: 64
      shuffle: True
      pin_memory: True
  valid:
    files:
      - data/proc/pretrain_drug/smiles_valid.txt
    loader:
      batch_size: 128
      shuffle: False
      pin_memory: True
  test:
    files:
      - data/proc/pretrain_drug/smiles_test.txt
    loader:
      batch_size: 128
      shuffle: False
      pin_memory: True
model:
  vocab_size: null  # set null to use tokenizer vocab size
  hidden_size: 128
  num_attention_heads: 8
  num_hidden_layers: 6
  intermediate_size: 1024
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  add_pooler: False
trainer:
  num_epochs: 5
  optimizer:
    lr: 1.e-4
    weight_decay: 1.e-3
  scheduler:
    name: constant_with_warmup  # "constant" "linear" "cosine" "cosine_with_restarts" "polynomial" "constant" "constant_with_warmup" "inverse_sqrt"
    params:
      num_training_steps: 72927  # 1555745//64+1=24309, *3~=72927
      num_warmup_steps: 2000
  print_per_steps: 1000
  eval_per_steps: 1000
  save_per_steps: 1000
  patience: 5
  max_keep: 5
  max_keep_best: 5
gpu: 0
model_dir: output/pretrain_drug/aa_encoder
