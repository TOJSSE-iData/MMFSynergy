tokenizer:
  model_dir: output/v3/pretrain_drug/smiles_tokenizer_bpe
  truncate: True
  max_length: 256
dataset:
  train:
    files:
      - data/proc/v8/pretrain_drug/smiles_train.txt
    loader:
      batch_size: 64
      shuffle: True
      pin_memory: True
model:
  vocab_size: null  # set null to use tokenizer vocab size
  hidden_size: 256
  num_attention_heads: 8
  num_hidden_layers: 6
  intermediate_size: 1024
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  add_pooler: False
  pooler_type: cls_before_pooler  # "cls", "cls_before_pooler", "avg", "avg_top2", "avg_first_last"
  temperature: 0.2
trainer:
  num_epochs: 1
  optimizer:
    lr: 3.e-5
    weight_decay: 1.e-3
  scheduler:
    name: constant_with_warmup  # "constant" "linear" "cosine" "cosine_with_restarts" "polynomial" "constant" "constant_with_warmup" "inverse_sqrt"
    params:
      num_training_steps: 27000  # 1728606//64+1=27010
      num_warmup_steps: 1000
  print_per_steps: 1000
  eval_per_steps: 1000
  save_per_steps: 1000
  patience: 5
  max_keep: 3
  max_keep_best: 3
gpu: 0
pretrain_model_path: output/v3/pretrain_drug/smiles_encoder_lyr3_lr0.0002_hdsz256_ir0.03/model_121000_0.5660.pt
model_dir: output/v3/pretrain_drug/smiles_encoder_simcse
